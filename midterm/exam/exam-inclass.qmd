---
title: ''
editor:
  render-on-save: true
format:
  pdf:
    documentclass: exam
    papersize: letter
    fontsize: 10pt
    include-in-header:
      - ./my-header.tex
      - ./my-macros.tex
    toc: false
    number-sections: true
    colorlinks: false
---


\noindent\makebox[0.75\textwidth]{Name:\enspace\hrulefill}

\vspace{5mm}

\begin{center}
    \LARGE{\textbf{In-class midterm, EC421}}
    \\ \large{120 points possible}
\end{center}

\vspace{5mm}

```{r, setup, include=FALSE}
# Packages
library(pacman)
p_load(fastverse, extrafont, latex2exp, ggplot2, scales)
```

## True or false (50 points; 20 questions)

**Note** In this section, select the correct answer (true or false). You do not need to explain your answer.

\begin{questions}

\tf Ordinary least squares (OLS) regression is biased for estimating the coefficients of a linear model when the disturbances are heteroskedastic.

\tf One of the ``standard'' assumptions for OLS regression is that the disturbances are uncorrelated with each other.

\tf If our disturbances have different variances, then we have a violation of exogeneity.

\tf In the regression model
$$
  \text{Income}_i = \beta_0 + \beta_1 \text{Education}_i + \beta_2 \text{Age}_i + u_i,
$$
the effect of education on income depends on the individual's age.

\tf The White test for heteroskedasticity for the above model would run the following regression:
$$
  e_i^2 = \beta_0 + \beta_1 \text{Education}_i + \beta_2 \text{Age}_i^2 + \beta_3 \text{Education}_i \times \text{Age} + v_i.
$$

\tf While OLS is biased when we violate exogeneity, it is still consistent.

\tf The square of a variable is equivalent to interacting the variable with itself.

\tf In the following model, if $\hat{\beta}_1 = 0.5$, then a one-unit increase in $X_i$ is associated with a 0.5-unit increase in $Y_i$:
$$
  \log(y)_i = \beta_0 + \beta_1 x_i + u_i.
$$

\newpage

\tf Weighted least squares (WLS) upweights individuals with higher variance disturbances and downweights individuals with lower variance disturbances.

\tf If the disturbance correlates with an explanatory variable, then we have a violation of the homoskedasticity assumption.

\tf An estimator can be consistent without being unbiased.

\tf Adding additional variables to a regression model will always increase the $R^2$.

\tf Disturbances are unobservable, while residuals are observable.

\tf Measurement error in the explanatory variable tends to bias the OLS estimator downward.

\tf For the Goldfeld-Quandt test, the null hypothesis is that the variances of the disturbances are equal across groups.

\tf Consistency tells us about the behavior of an estimator when we take an infinite number of samples with the same sample size.

\tf In the following regression model, the expected income for a non-female student is $\beta_0 + \beta_1$
$$
  \text{Income}_i = \beta_0 + \beta_1 \text{Female}_i + \beta_2 \text{Student} + \beta_3 \text{Female}_i \times \text{Student}_i + u_i.
$$

\tf A p-value of 0.5 means that we can reject the null hypothesis that the coefficient is equal to zero at the 5\% level.

\tf One problem of the Goldfeld-Quandt test for heteroskedasticity is that it fails to detect certain patterns of heteroskedasticity.

\tf Omitted-variable bias occurs when we omit a variable from a regression.

\end{questions}

\newpage

## Multiple choice (20 points; 5 questions)

**Note** In this section, check ($\checkmark$ or $\times$) \textbf{all} correct answers. You do not need to explain your answer.

\begin{questions}

\setcounter{question}{20}

\mc \\ Which of the following statements are part of the ``standard'' assumptions for OLS regression?

\vspace{\stretch{1}}
\begin{oneparcheckboxes} 
  \choice $E[u_i | X_i] = 0$
  \choice $\text{Var}(u_i) = 0$
  \choice $\text{Var}(X_i) > 0$
  \choice $\text{Cov}(u_i, u_j) = 0$
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ In the presence of heteroskedasticity, which of the following statements are true?

\vspace{\stretch{1}}
\begin{oneparcheckboxes} 
  \choice OLS is unbiased for the coefficients. \newline
  \choice WLS is biased for the coefficients. \newline
  \choice WLS is the best linear unbiased estimator. \newline
  \choice OLS is biased for the standard errors. \newline
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ Which of the following scenarios necessarily causes OLS to be biased for the coefficients?

\vspace{\stretch{1}}
\begin{oneparcheckboxes} 
  \choice heteroskedasticity
  \choice correlated disturbances
  \choice violating exogeneity
  \choice measurement error
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ Which of the following models \textit{violates} OLS's requirement of linearity?

\vspace{\stretch{1}}
\begin{oneparcheckboxes} 
  \choice $\log(y) = \beta_0 + \beta_1 \log(x_1) + \beta_2 \log(x_2) + u$ \newline
  \choice $y = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + u}$ \newline
  \choice $y = \beta_0 + \beta_1 x_1^{\beta_2} + \beta_3 x_2 + u$ \newline
  \choice $y = \beta_0 + \beta_1 x_1 \beta_2 x_1^2 + \beta_3 x_2 + \beta_4 x_2^2 + u$ \newline
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ What can we ``do'' in the presence of heteroskedasticity?

\vspace{\stretch{1}}
\begin{oneparcheckboxes} 
  \choice Check the specification.
  \choice Look for omitted variables. \newline
  \choice Use het.-robust standard errors.
  \choice Estimate the model using WLS.
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\end{questions}

## Short answer (50 points; 10 questions)

\textbf{Note} In this section, briefly answer the questions/prompts in 1--3 short (and complete) sentences. We will deduct points for excessively long answers.

\begin{questions}

\setcounter{question}{25}

\short Explain how OLS regression defines the ``best-fit'' line.

\vspace{\stretch{1}}

\short Define the concept of a \textit{standard error}.

\vspace{\stretch{1}}

\short Explain how the phrase \textit{correlation is not causation} relates to the concept of \textit{omitted-variable bias}.

\vspace{\stretch{1}} 

\newpage

\short Write down a simple linear regression model (actual variables; not just $y$ and $x$), and provide an example of a variable that could cause omitted variable bias. Explain how the variable satisfies the requirements for omitted-variable bias.

\vspace{\stretch{1}} 

\short In class we showed that for included regressor $x_{1}$ and excluded regressor $x_{2}$, the coefficient on $x_{1}$ has probability limit
$$
  \text{plim}\, \hat{\beta}_1 = \beta_1 + \frac{\text{Cov}(x_{1}, x_{2})}{\text{Var}(x_{1})}
$$
Using this formula, explain which direction you would expect the OLS estimator to be biased when we regress \textit{income} on \textit{education} and omit \textit{ability}.

\vspace{\stretch{1}}

\short Compare and contrast consistency and unbiasedness.

\vspace{\stretch{1}}

\newpage

\short For the regression model below, suppose we estimate $\hat{\beta}_0 = 9.4$ and $\hat{\beta}_1 = -0.5$. Interpret the slope coefficient in the context of the model.
$$
  \log(\text{Quantity}_{i}) = \beta_0 + \beta_1 \log(\text{Price}_{i}) + u_{i}
$$

\vspace{\stretch{1}}

\short For the regression model below, suppose we estimate $\hat{\beta}_{0} = 55.4$, $\hat{\beta}_{1} = 0.3$, $\hat{\beta}_{2} = 12.1$, and $\hat{\beta}_{3} = 0.1$. Interpret the coefficient on the interaction term.
$$
  (\text{Years Lived})_i = \beta_0 + \beta_1 \text{Income}_i + \beta_2 \text{Female}_i + \beta_3 \text{Income}_i \times \text{Female}_i + u_i
$$
\textit{Note:} \textit{Income} is measured in thousands of dollars; \textit{Female} is a binary indicator variable.

\vspace{\stretch{1}}

\newpage

\short Imagine you are running the White test for heteroskedasticity. You notice that the coefficient on $x_{1}^2$ is statistically significant. What does this tell you about the presence of heteroskedasticity? Explain.

\vspace{\stretch{1}}

\short Draw an example of a scatterplot that would violate the assumption of exogeneity. Briefly explain why the scatterplot violates the assumption.

\vspace{\stretch{1}}

\end{questions}
